{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Hierarchical prototypical networks\n",
    "### FC-100 training/testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b146a710ee0acefd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "295b0e69343cdbae"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from CPEA.utils import level2_mapping\n",
    "\n",
    "from dataloader.samplers import CategoriesSampler\n",
    "from utils import ensure_path, Averager, compute_confidence_interval\n",
    "from tensorboardX import SummaryWriter\n",
    "from types import SimpleNamespace\n",
    "import gc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-05T16:14:41.155036900Z",
     "start_time": "2024-09-05T16:14:37.397998200Z"
    }
   },
   "id": "94c0fdbe549fb2bd"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-05T16:14:42.901636900Z",
     "start_time": "2024-09-05T16:14:42.776432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "91"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = SimpleNamespace(\n",
    "    max_epoch=2,\n",
    "    way=5,\n",
    "    test_way=5,\n",
    "    shot=1,\n",
    "    query=15,\n",
    "    lr=0.00001,\n",
    "    lr_mul=100,\n",
    "    step_size=5,\n",
    "    gamma=0.5,\n",
    "    model_type='small',\n",
    "    dataset='FC100',\n",
    "    init_weights='./initialization/fc100/checkpoint1600.pth',\n",
    "    gpu='0',\n",
    "    exp='CPEA'\n",
    ")\n",
    "save_path = '-'.join([args.exp, args.dataset, args.model_type])\n",
    "args.save_path = osp.join('./results', save_path)\n",
    "ensure_path(args.save_path)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15481258d7bc13f7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from dataloader.hfc100 import FC100 as Dataset\n",
    "\n",
    "trainset = Dataset('train', args)\n",
    "train_sampler = CategoriesSampler(trainset.label, 10, args.way, args.shot + args.query)\n",
    "train_loader = DataLoader(dataset=trainset, batch_sampler=train_sampler, num_workers=8, pin_memory=True)\n",
    "\n",
    "valset = Dataset('val', args)\n",
    "val_sampler = CategoriesSampler(valset.label, 10, args.test_way, args.shot + args.query)\n",
    "val_loader = DataLoader(dataset=valset, batch_sampler=val_sampler, num_workers=8, pin_memory=True)\n",
    "\n",
    "testset = Dataset('test', args)\n",
    "test_sampler = CategoriesSampler(testset.label, 10, args.test_way, args.shot + args.query)\n",
    "test_loader = DataLoader(dataset=testset, batch_sampler=test_sampler, num_workers=8, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-05T16:14:46.187889600Z",
     "start_time": "2024-09-05T16:14:44.594718200Z"
    }
   },
   "id": "ff8aae758d8a5422"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62eeb02f520b848e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from models.protonet import ProtoNet\n",
    "\n",
    "backbone = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "backbone.fc = nn.Flatten()\n",
    "\n",
    "classifier = ProtoNet(backbone)\n",
    "\n",
    "backbone_optimizer = torch.optim.Adam([{'params': backbone.parameters()}], lr=args.lr, weight_decay=0.001)\n",
    "backbone_scheduler = torch.optim.lr_scheduler.StepLR(backbone_optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "\n",
    "classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=args.lr * args.lr_mul, weight_decay=0.001)\n",
    "classifier_scheduler = torch.optim.lr_scheduler.StepLR(classifier_optimizer, step_size=args.step_size, gamma=args.gamma)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-05T16:14:48.349822Z",
     "start_time": "2024-09-05T16:14:47.394783100Z"
    }
   },
   "id": "23886e570fda22c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize pretrained model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23ca82d090f124e6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def save_model(name):\n",
    "    torch.save(dict(params=backbone.state_dict()), osp.join(args.save_path, name + '.pth'))\n",
    "    torch.save(dict(params=classifier.state_dict()),\n",
    "               osp.join(args.save_path, name + '_classifier.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-05T16:14:50.691255400Z",
     "start_time": "2024-09-05T16:14:50.670297800Z"
    }
   },
   "id": "cc57e728743c1cbb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize logging"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa17a288220d43ed"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "global_count = 0\n",
    "writer = SummaryWriter(comment=args.save_path)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trlog = {'args': vars(args), 'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'max_acc': 0.0,\n",
    "         'max_acc_epoch': 0}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-05T16:14:59.941189300Z",
     "start_time": "2024-09-05T16:14:59.929194300Z"
    }
   },
   "id": "1749d40511748605"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2e04b83e717a5f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "9"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-05T16:15:01.240411900Z",
     "start_time": "2024-09-05T16:15:01.139790600Z"
    }
   },
   "id": "3c62709f5f4d5cf8"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def create_mapping(labels):\n",
    "    labels_mapping = {}\n",
    "    current_index = 0\n",
    "    \n",
    "    for label in labels:\n",
    "        if label not in labels_mapping.keys():\n",
    "            labels_mapping[label] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "    return labels_mapping"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-05T16:15:02.621485800Z",
     "start_time": "2024-09-05T16:15:02.595969200Z"
    }
   },
   "id": "7e0baa99c0bdfdd9"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "backbone_dict = {}\n",
    "classifier_dict = {}\n",
    "\n",
    "for i in range(1 + len(level2_mapping)):\n",
    "    backbone = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    backbone.fc = nn.Flatten()\n",
    "    backbone_dict[i] = backbone\n",
    "    classifier_dict[i] = ProtoNet(backbone)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-05T16:15:14.610384500Z",
     "start_time": "2024-09-05T16:15:03.755134600Z"
    }
   },
   "id": "833b324443ac2dde"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25276\\158097485.py\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mrandom\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mloss_fn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mCrossEntropyLoss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmanual_seed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for epoch in range(1, args.max_epoch + 1):\n",
    "    print(f\"epoch {epoch}\")\n",
    "    # get parent level model\n",
    "    backbone = backbone_dict[0]\n",
    "    classifier = classifier_dict[0]\n",
    "    \n",
    "    backbone.train()\n",
    "    classifier.train()\n",
    "    tl = Averager()\n",
    "    ta = Averager()\n",
    "\n",
    "    for i, batch in enumerate(train_loader, 1):\n",
    "        print(f\"batch {i}\")\n",
    "        loss = 0\n",
    "        backbone.zero_grad()\n",
    "        classifier.zero_grad()\n",
    "        global_count = global_count + 1\n",
    "\n",
    "        data, labels, parent_names = batch\n",
    "        parent_labels = [level2_mapping[label_name] for label_name in parent_names[0]]\n",
    "        \n",
    "        p = args.shot * args.test_way\n",
    "        support_data, query_data = data[:p], data[p:]\n",
    "        support_labels, query_labels = labels[:p], labels[p:]\n",
    "        parent_support_labels, parent_query_labels = parent_labels[:p], parent_labels[p:]\n",
    "        \n",
    "        parent_labels_support_mapping = create_mapping(parent_support_labels)\n",
    "                \n",
    "        mapped_parent_support_labels = torch.Tensor([parent_labels_support_mapping[label] for label in np.array(parent_support_labels)])        \n",
    "        mapped_parent_query_labels = torch.Tensor([parent_labels_support_mapping[label] for label in np.array(parent_query_labels)])        \n",
    "        \n",
    "        # mapped_query_labels = torch.arange(args.test_way).repeat(args.query).long()\n",
    "        # scores = classifier(support_data, support_labels, query_data, n_way)\n",
    "        # loss = loss_fn(scores, mapped_query_labels)\n",
    "        \n",
    "        n_way_parent = len(mapped_parent_support_labels.unique())\n",
    "        scores_parent = classifier(support_data, mapped_parent_support_labels, query_data, n_way_parent)\n",
    "        loss_parent = loss_fn(scores_parent, mapped_parent_query_labels.long())\n",
    "        pred_parent = torch.argmax(scores_parent, dim=1)\n",
    "        \n",
    "        loss += loss_parent\n",
    "        \n",
    "        for ind, query_image in enumerate(query_data):\n",
    "            print(ind)\n",
    "            sub_backbone = backbone_dict[ind]\n",
    "            sub_backbone.train()\n",
    "            sub_backbone.zero_grad()\n",
    "            \n",
    "            sub_classifier = classifier_dict[ind]\n",
    "            sub_classifier.train()\n",
    "            sub_classifier.zero_grad()\n",
    "            \n",
    "            subclass_support_images = support_data[mapped_parent_support_labels == pred_parent[ind]]\n",
    "            subclass_support_labels = support_labels[mapped_parent_support_labels == pred_parent[ind]]\n",
    "            \n",
    "            subclass_support_mapping = create_mapping(np.array(subclass_support_labels))\n",
    "            \n",
    "            mapped_subclass_support_labels = torch.Tensor([subclass_support_mapping[label] for label in np.array(subclass_support_labels)]) \n",
    "            mapped_subclass_query_label = torch.Tensor([subclass_support_mapping[query_labels[ind].item()]])\n",
    "\n",
    "            n_way_subclass = len(mapped_subclass_support_labels.unique())\n",
    "            scores_subclass = sub_classifier(subclass_support_images, mapped_subclass_support_labels, query_data[ind].unsqueeze(0), n_way_subclass)\n",
    "            pred_subclass = torch.argmax(scores_subclass, dim=1)\n",
    "\n",
    "            loss_subclass = loss_fn(scores_subclass, mapped_subclass_query_label.long())\n",
    "            loss += loss_subclass\n",
    "            \n",
    "        # pred = torch.argmax(scores, dim=1)\n",
    "        # acc = (pred == mapped_query_labels).type(torch.FloatTensor).mean().item()\n",
    "        writer.add_scalar('data/loss', float(loss), global_count)\n",
    "        # writer.add_scalar('data/acc', float(acc), global_count)\n",
    "        print('epoch {}, train {}/{}, loss={:.4f} acc={:.4f}'.format(epoch, i, len(train_loader), loss.item(), -1))\n",
    "\n",
    "        # tl.add(loss.item())\n",
    "        # ta.add(acc)\n",
    "\n",
    "        loss.backward()\n",
    "        backbone_optimizer.step()\n",
    "        classifier_optimizer.step()\n",
    "    '''\n",
    "    backbone_scheduler.step()\n",
    "    classifier_scheduler.step()\n",
    "\n",
    "    tl = tl.item()\n",
    "    ta = ta.item()\n",
    "\n",
    "    backbone.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    vl = Averager()\n",
    "    va = Averager()\n",
    "\n",
    "    print('best epoch {}, best val acc={:.4f}'.format(trlog['max_acc_epoch'], trlog['max_acc']))\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader, 1):\n",
    "            data, labels = batch\n",
    "            p = args.shot * args.test_way\n",
    "            support_data, query_data = data[:p], data[p:]\n",
    "            support_labels, query_labels = labels[:p], labels[p:]\n",
    "            mapped_query_labels = torch.arange(args.test_way).repeat(args.query).long()\n",
    "\n",
    "            scores = classifier(support_data, support_labels, query_data\n",
    "                                               )\n",
    "            loss = loss_fn(scores, mapped_query_labels)\n",
    "            pred = torch.argmax(scores, dim=1)\n",
    "            acc = (pred == mapped_query_labels).type(torch.FloatTensor).mean().item()\n",
    "            vl.add(loss.item())\n",
    "            va.add(acc)\n",
    "\n",
    "    vl = vl.item()\n",
    "    va = va.item()\n",
    "    writer.add_scalar('data/val_loss', float(vl), epoch)\n",
    "    writer.add_scalar('data/val_acc', float(va), epoch)\n",
    "    print('epoch {}, val, loss={:.4f} acc={:.4f}'.format(epoch, vl, va))\n",
    "\n",
    "    if va >= trlog['max_acc']:\n",
    "        trlog['max_acc'] = va\n",
    "        trlog['max_acc_epoch'] = epoch\n",
    "        save_model('max_acc')\n",
    "\n",
    "    trlog['train_loss'].append(tl)\n",
    "    trlog['train_acc'].append(ta)\n",
    "    trlog['val_loss'].append(vl)\n",
    "    trlog['val_acc'].append(va)\n",
    "\n",
    "    torch.save(trlog, osp.join(args.save_path, 'trlog'))\n",
    "    save_model('epoch-last')\n",
    "    '''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-05T16:26:10.066157600Z",
     "start_time": "2024-09-05T16:26:08.455986100Z"
    }
   },
   "id": "c5f29ba2027d8316"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d1f2e1024fa6dff"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "ProtoNet(\n  (backbone): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Flatten(start_dim=1, end_dim=-1)\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trlog = torch.load(osp.join(args.save_path, 'trlog'))\n",
    "\n",
    "backbone.load_state_dict(torch.load(osp.join(args.save_path, 'max_acc' + '.pth'))['params'])\n",
    "backbone.eval()\n",
    "\n",
    "classifier.load_state_dict(\n",
    "    torch.load(osp.join(args.save_path, 'max_acc' + '_classifier.pth'))['params'])\n",
    "classifier.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-03T18:32:34.018148600Z",
     "start_time": "2024-09-03T18:32:33.893138500Z"
    }
   },
   "id": "e683b7948073d4b5"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: acc 36.00(36.00)\n",
      "batch 2: acc 28.00(20.00)\n",
      "batch 3: acc 28.44(29.33)\n",
      "batch 4: acc 31.67(41.33)\n",
      "batch 5: acc 30.13(24.00)\n",
      "batch 6: acc 28.67(21.33)\n",
      "batch 7: acc 28.57(28.00)\n",
      "batch 8: acc 27.33(18.67)\n",
      "batch 9: acc 28.00(33.33)\n",
      "batch 10: acc 28.40(32.00)\n",
      "Val Best Epoch 1, Acc 0.2827\n",
      "Test Acc 28.4000 + 4.3737\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_acc_record = np.zeros((10,))\n",
    "ave_acc = Averager()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader, 1):\n",
    "        data, labels, parent_labels = batch\n",
    "        p = args.shot * args.test_way\n",
    "        support_data, query_data = data[:p], data[p:]\n",
    "        support_labels, query_labels = labels[:p], labels[p:]\n",
    "        ls = torch.arange(args.test_way).repeat(args.query).long()\n",
    "\n",
    "        scores = classifier(support_data, support_labels, query_data\n",
    "                                           )\n",
    "        loss = loss_fn(scores, ls)\n",
    "        pred = torch.argmax(scores, dim=1)\n",
    "        acc = (pred == ls).type(torch.FloatTensor).mean().item()\n",
    "\n",
    "        ave_acc.add(acc)\n",
    "        test_acc_record[i - 1] = acc\n",
    "        print('batch {}: acc {:.2f}({:.2f})'.format(i, ave_acc.item() * 100, acc * 100))\n",
    "\n",
    "m, pm = compute_confidence_interval(test_acc_record)\n",
    "print('Val Best Epoch {}, Acc {:.4f}'.format(trlog['max_acc_epoch'], trlog['max_acc']))\n",
    "print('Test Acc {:.4f} + {:.4f}'.format(m * 100, pm * 100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-03T18:35:26.539735400Z",
     "start_time": "2024-09-03T18:34:07.467353400Z"
    }
   },
   "id": "9c84b74f38b6ad4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
